{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# utile folder ----> util\n","\n","import torch\n","import json\n","from torch.utils.data import Dataset\n","import numpy as np\n","from transformers import AutoTokenizer, RobertaTokenizerFast\n","\n","#from utils.utils import match_labels\n","\n","\n","\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","############################################################\n","#                                                          #\n","#                      DATASET CLASS                       #\n","#                   LeglNERTokenDataset                    #\n","############################################################\n","class LegalNERTokenDataset(Dataset):\n","\n","    def __init__(self, dataset_path, model_path, labels_list=None, split=\"train\", use_roberta=False):\n","        self.data = json.load(open(dataset_path))\n","        self.split = split\n","        self.use_roberta = use_roberta\n","        if self.use_roberta:     ## Load the right tokenizer\n","            self.tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\") # use roberta tokenization for tokenize the data / text\n","        else:\n","            self.tokenizer = AutoTokenizer.from_pretrained(model_path)  # if it was roberta do it else use Auto Tokenizer to do that\n","\n","        self.labels_list = sorted(labels_list + [\"O\"])[::-1]\n","\n","        if self.labels_list is not None: # it is not none with [\"O\" , \"I-Z\" , ..... ] reverted\n","             self.labels_to_idx = dict(\n","                zip(sorted(self.labels_list)[::-1], range(len(self.labels_list)))\n","            )\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        text = item[\"data\"][\"text\"]\n","\n","        ## Get the annotations\n","        annotations = [\n","            { # to pin point the location o fthe laz\n","                \"start\": v[\"value\"][\"start\"],\n","                \"end\": v[\"value\"][\"end\"],\n","                \"labels\": v[\"value\"][\"labels\"][0],\n","            }\n","            for v in item[\"annotations\"][0][\"result\"]\n","        ]\n","\n","        ## Tokenize the text\n","        if not self.use_roberta:\n","            inputs = self.tokenizer(    # if the roberta tokenizer is active, So the inputs has roberta tokenizer output\n","                text,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                verbose=False\n","                )\n","        else:\n","            inputs = self.tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                verbose=False,\n","                padding='max_length' # pay attention to this section latter for data pre-processing\n","            )\n","\n","            # after the tokenizing the data we need to do some padding actions to make a better perfomance\n","\n","\n","\n","\n","\n","        ## Match the labels\n","        aligned_labels = match_labels(inputs, annotations) #### what is the result of this function\n","        # the result of this function is\n","        aligned_labels = [self.labels_to_idx[l] for l in aligned_labels] # this section gives me a list of various numbers which are in labels_to_idx lke 4,2,8,5,1,0,....\n","        # aligned_labels from [\"O\" , \"B-Practitioner\" , .....] will be converted to he number of labels_list we set from 0 to a Number\n","        # ---> [0,4,2,7,1,8,3]\n","        inputs[\"input_ids\"] = inputs[\"input_ids\"].squeeze(0).long()\n","        inputs[\"attention_mask\"] = inputs[\"attention_mask\"].squeeze(0).long()\n","        if not self.use_roberta:\n","            inputs[\"token_type_ids\"] = inputs[\"token_type_ids\"].squeeze(0).long()\n","\n","        ## Get the labels\n","        if self.labels_list:\n","            labels = torch.tensor(aligned_labels).squeeze(-1).long()\n","\n","            if labels.shape[0] < inputs[\"attention_mask\"].shape[0]:\n","                pad_x = torch.zeros((inputs[\"input_ids\"].shape[0],))\n","                pad_x[: labels.size(0)] = labels\n","                inputs[\"labels\"] = aligned_labels\n","            else:\n","                inputs[\"labels\"] = labels[: inputs[\"attention_mask\"].shape[0]]\n","\n","        return inputs\n","\n","\n","#### editing this section for preprocessing\n","\n","\n","# --> match label function\n","\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import numpy as np\n","from nervaluate import Evaluator\n","\n","\n","############################################################\n","#                                                          #\n","#                  LABELS MATCHING FUNCTION                #\n","# Should be edited for preprocessing and enhance F1-scores #\n","############################################################\n","def match_labels(tokenized_input, annotations):\n","\n","    # Make a list to store our labels the same length as our tokens\n","    aligned_labels = [\"O\"] * len(\n","        tokenized_input[\"input_ids\"][0]\n","    ) #.        ------> for example if len aligned_labels is 7 so we have such list --> inout_ids : [  101, 17662,  2227, 19081,  2003,  2307,   999,   102] ---> [\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",]\n","\n","\n","      # Example : tensor([[ 101, 7592, 1010, 2047, 2259, 2103, 2003, 1037, 2307, 2103, 1999, 1996, 2088, 1012,  102]]),\n","      # aligned_labels  = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] --> 14 * 0\n","      # annotation\n","\n","    # Loop through the annotations\n","    for anno in annotations:\n","\n","      # anno ---> start / end / value\n","\n","        previous_tokens = None\n","\n","        # Loop through the characters in the annotation\n","        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n","\n","            token_ix = tokenized_input.char_to_token(char_ix) # convert the character to token that the output is a number\n","\n","            # White spaces have no token and will return None\n","            if token_ix is not None:\n","\n","                # If the token is a continuation of the previous token, we label it as \"I\"\n","                if previous_tokens is not None:\n","                    aligned_labels[token_ix] = (\n","                        \"I-\" + anno[\"labels\"]\n","                        if aligned_labels[token_ix] == \"O\"\n","                        else aligned_labels[token_ix]\n","                    )\n","\n","                # If the token is not a continuation of the previous token, we label it as \"B\"\n","                else:\n","                    aligned_labels[token_ix] = \"B-\" + anno[\"labels\"]\n","                    previous_tokens = token_ix\n","\n","    return aligned_labels\n","\n","\n","\n","###################\n","\n","#1- modifying both LegalNER and Matche labeling for preprocessing and enhance F1-Scores\n","#2- change the number of input parameters\n","\n","\n","\n","\n","###################\n","\n","import os\n","import json\n","import numpy as np\n","from argparse import ArgumentParser\n","from nervaluate import Evaluator\n","\n","from transformers import AutoModelForTokenClassification\n","from transformers import Trainer, DefaultDataCollator, TrainingArguments\n","\n","# from utils.dataset import LegalNERTokenDataset ***************\n","\n","#import LegalNERTokenDataset\n","#import datasets\n","\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","\n","############################################################\n","#                                                          #\n","#                           MAIN                           #\n","#                                                          #\n","############################################################\n","if __name__ == \"__main__\":\n","\n","    parser = ArgumentParser(description=\"Training of LUKE model\")\n","    parser.add_argument(\n","        \"--ds_train_path\",\n","        help=\"Path of train dataset file\",\n","        #default=\"data/NER_TRAIN/NER_TRAIN_ALL.json\", ## we. should change the path  ---> I think ALL is the combination of that !!!!\n","        default = \"//content//drive//MyDrive//DeepLearning_Project//NER_TRAIN//NER_TRAIN_JUDGEMENT.json\",\n","        required=False,\n","        type=str,\n","    )\n","    parser.add_argument(\n","        \"--ds_valid_path\",\n","        help=\"Path of validation dataset file\",\n","        #default=\"data/NER_DEV/NER_DEV_ALL.json\",\n","        default = \"//content//drive//MyDrive//DeepLearning_Project//NER_DEV//NER_DEV_JUDGEMENT.json\",\n","        required=False,\n","        type=str,\n","    )\n","\n","    ## Pay attention that we have just used Judgment for Train and DEV !!!!! **** !!!!!\n","\n","    parser.add_argument(\n","        \"--output_folder\",\n","        help=\"Output folder\",\n","        #default=\"results/\",\n","        default = \"//content//drive//MyDrive//DeepLearning_Project//\",\n","        required=False,\n","        type=str,\n","    )\n","    parser.add_argument(\n","        \"--batch\",\n","        help=\"Batch size\",\n","        default=1,\n","        required=False,\n","        type=int,\n","    )\n","    parser.add_argument(\n","        \"--num_epochs\",\n","        help=\"Number of training epochs\",\n","        default=5,\n","        required=False,\n","        type=int,\n","    )\n","    parser.add_argument(\n","        \"--lr\",\n","        help=\"Learning rate\",\n","        default=1e-5,\n","        required=False,\n","        type=float,\n","    )\n","    parser.add_argument(\n","        \"--weight_decay\",\n","        help=\"Weight decay\",\n","        default=0.01,\n","        required=False,\n","        type=float,\n","    )\n","    parser.add_argument(\n","        \"--warmup_ratio\",\n","        help=\"Warmup ratio\",\n","        default=0.06,\n","        required=False,\n","        type=float,\n","    )\n","\n","    args = parser.parse_args()\n","\n","    # here is to make ready the input arguments for the model !!!!\n","    ## Parameters\n","    ds_train_path = args.ds_train_path  # e.g., 'data/NER_TRAIN/NER_TRAIN_ALL.json'\n","    ds_valid_path = args.ds_valid_path  # e.g., 'data/NER_DEV/NER_DEV_ALL.json'\n","    output_folder = args.output_folder  # e.g., 'results/'\n","    batch_size = args.batch             # e.g., 256 for luke-based, 1 for bert-based\n","    num_epochs = args.num_epochs        # e.g., 5\n","    lr = args.lr                        # e.g., 1e-4 for luke-based, 1e-5 for bert-based\n","    weight_decay = args.weight_decay    # e.g., 0.01\n","    warmup_ratio = args.warmup_ratio    # e.g., 0.06\n","\n","\n","\n","    ## Define the labels, Actually these are the labels for Named Entity Recognition\n","    original_label_list = [\n","        \"COURT\",\n","        \"PETITIONER\",\n","        \"RESPONDENT\",\n","        \"JUDGE\",\n","        \"DATE\",\n","        \"ORG\",\n","        \"GPE\",\n","        \"STATUTE\",\n","        \"PROVISION\",\n","        \"PRECEDENT\",\n","        \"CASE_NUMBER\",\n","        \"WITNESS\",\n","        \"OTHER_PERSON\",\n","        \"LAWYER\"\n","    ]\n","    labels_list = [\"B-\" + l for l in original_label_list]\n","    labels_list += [\"I-\" + l for l in original_label_list]\n","    # label_list is the list with these items ----> #['B-COURT',\n","    #  'B-PETITIONER',\n","    #  'B-RESPONDENT',\n","    #  'B-JUDGE',\n","    #  'B-DATE',\n","    #  'B-ORG',\n","    #  'B-GPE',\n","    #  'B-STATUTE',\n","    #  'B-PROVISION',\n","    #  'B-PRECEDENT',\n","    #  'B-CASE_NUMBER',\n","    #  'B-WITNESS',\n","    #  'B-OTHER_PERSON',\n","    #  'B-LAWYER',\n","    #  'I-COURT',\n","    #  'I-PETITIONER',\n","    #  'I-RESPONDENT',\n","    #  'I-JUDGE',\n","    #  'I-DATE',\n","    #  'I-ORG',\n","    #  'I-GPE',\n","    #  'I-STATUTE',\n","    #  'I-PROVISION',\n","    #  'I-PRECEDENT',\n","    #  'I-CASE_NUMBER',\n","    #  'I-WITNESS',\n","    #  'I-OTHER_PERSON',\n","    #  'I-LAWYER']\n","    num_labels = len(labels_list) + 1\n","    # there is no need to specify the data as O --> outside label list\n","\n","\n","\n","\n","    ## Compute metrics, this function is considered as Evaluator to assess the performance of the model\n","    # IDX to LABELS\n","    # LABELS to IDX\n","    def compute_metrics(pred):\n","\n","        # Preds\n","        predictions = np.argmax(pred.predictions, axis=-1)\n","        predictions = np.concatenate(predictions, axis=0) ### --> row by row concatenates the numbers of data\n","        prediction_ids = [[idx_to_labels[p] if p != -100 else \"O\" for p in predictions]]\n","        # put zero if the number is eqaul to -100 ---> for index to label\n","\n","        # Labels\n","        labels = pred.label_ids\n","        labels = np.concatenate(labels, axis=0)\n","        labels_ids = [[idx_to_labels[p] if p != -100 else \"O\" for p in labels]]\n","        unique_labels = list(set([l.split(\"-\")[-1] for l in list(set(labels_ids[0]))]))\n","        unique_labels.remove(\"O\")\n","\n","        # Evaluator\n","        evaluator = Evaluator(\n","            labels_ids, prediction_ids, tags=unique_labels, loader=\"list\"\n","        )\n","        results, results_per_tag = evaluator.evaluate()\n","\n","\n","\n","        return {\n","            \"f1-type-match\": 2\n","            * results[\"ent_type\"][\"precision\"]\n","            * results[\"ent_type\"][\"recall\"]\n","            / (results[\"ent_type\"][\"precision\"] + results[\"ent_type\"][\"recall\"] + 1e-9),\n","            \"f1-partial\": 2\n","            * results[\"partial\"][\"precision\"]\n","            * results[\"partial\"][\"recall\"]\n","            / (results[\"partial\"][\"precision\"] + results[\"partial\"][\"recall\"] + 1e-9),\n","            \"f1-strict\": 2\n","            * results[\"strict\"][\"precision\"]\n","            * results[\"strict\"][\"recall\"]\n","            / (results[\"strict\"][\"precision\"] + results[\"strict\"][\"recall\"] + 1e-9),\n","            \"f1-exact\": 2\n","            * results[\"exact\"][\"precision\"]\n","            * results[\"exact\"][\"recall\"]\n","            / (results[\"exact\"][\"precision\"] + results[\"exact\"][\"recall\"] + 1e-9),\n","        } # end of the function\n","\n","    ## Define the models\n","    model_paths = [\n","        \"dslim/bert-large-NER\",                     # ft on NER\n","        \"Jean-Baptiste/roberta-large-ner-english\",  # ft on NER\n","        \"nlpaueb/legal-bert-base-uncased\",          # ft on Legal Domain\n","        \"saibo/legal-roberta-base\",                 # ft on Legal Domain\n","        \"nlpaueb/bert-base-uncased-eurlex\",         # ft on Eurlex\n","        \"nlpaueb/bert-base-uncased-echr\",           # ft on ECHR\n","        \"studio-ousia/luke-base\",                   # LUKE base\n","        \"studio-ousia/luke-large\",                  # LUKE large\n","    ]\n","\n","\n","    ### the program will start from here *************########\n","    for model_path in model_paths:\n","\n","        print(\"MODEL: \", model_path)\n","\n","        ## Define the train and test datasets\n","        use_roberta = False\n","        if \"luke\" in model_path or \"roberta\" in model_path:\n","            use_roberta = True\n","\n","        train_ds = LegalNERTokenDataset(\n","            ds_train_path,\n","            model_path,\n","            labels_list=labels_list,\n","            split=\"train\",\n","            use_roberta=use_roberta\n","        )\n","\n","        val_ds = LegalNERTokenDataset(\n","            ds_valid_path,\n","            model_path,\n","            labels_list=labels_list,\n","            split=\"val\",\n","            use_roberta=use_roberta\n","        )\n","\n","\n","        ## Define the model\n","        model = AutoModelForTokenClassification.from_pretrained(\n","            model_path,\n","            num_labels=num_labels,\n","            ignore_mismatched_sizes=True\n","        )\n","\n","\n","\n","        ## Map the labels\n","        idx_to_labels = {v[1]: v[0] for v in train_ds.labels_to_idx.items()}\n","        # in the dictionary the labels will be converted to the index Number\n","        # As an example : Ali --> 343.3434\n","\n","        ## Output folder\n","        new_output_folder = os.path.join(output_folder, 'all')\n","        new_output_folder = os.path.join(new_output_folder, model_path)\n","        if not os.path.exists(new_output_folder):\n","            os.makedirs(new_output_folder)\n","\n","        ## Training Arguments\n","        training_args = TrainingArguments(\n","            output_dir=new_output_folder,\n","            num_train_epochs=num_epochs,\n","            learning_rate=lr,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=batch_size,\n","            gradient_accumulation_steps=1,\n","            gradient_checkpointing=True,\n","            warmup_ratio=warmup_ratio,\n","            weight_decay=weight_decay,\n","            evaluation_strategy=\"epoch\",\n","            save_strategy=\"epoch\",\n","            load_best_model_at_end=False,\n","            save_total_limit=2,\n","            fp16=False,\n","            fp16_full_eval=False,\n","            metric_for_best_model=\"f1-strict\",\n","            dataloader_num_workers=4,\n","            dataloader_pin_memory=True,\n","        )\n","\n","        ## Collator\n","        data_collator = DefaultDataCollator()\n","\n","        ## Trainer\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=train_ds,\n","            eval_dataset=val_ds,\n","            compute_metrics=compute_metrics,\n","            data_collator=data_collator,\n","        )\n","\n","        ## Train the model and save it\n","        trainer.train()\n","        trainer.save_model(output_folder)\n","        trainer.evaluate()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"id":"P8hBmGj3gr2s","executionInfo":{"status":"error","timestamp":1702553610721,"user_tz":-60,"elapsed":3066,"user":{"displayName":"Kharamani Reza","userId":"06083874175566094280"}},"outputId":"1d8907af-ed87-4b74-fc61-8bba2c6d996e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["usage: colab_kernel_launcher.py [-h] [--ds_train_path DS_TRAIN_PATH]\n","                                [--ds_valid_path DS_VALID_PATH] [--output_folder OUTPUT_FOLDER]\n","                                [--batch BATCH] [--num_epochs NUM_EPOCHS] [--lr LR]\n","                                [--weight_decay WEIGHT_DECAY] [--warmup_ratio WARMUP_RATIO]\n","colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-4b0f6ef6-85c4-4e03-8c80-58bc95be4e79.json\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xW67ufTRFC9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8I8Lu1t5c6Le"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_nzANdYQf0R7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eaMUuj-UgeMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cqw1AvrcrOaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LzULM8y6sKy5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"L8emus8dsoAI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vdg8Hgg0yLF7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XWOMMfsz1-Mj"},"execution_count":null,"outputs":[]}]}